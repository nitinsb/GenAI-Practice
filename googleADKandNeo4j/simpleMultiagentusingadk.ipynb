{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c142a74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.models.lite_llm import LiteLlm # For OpenAI support\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.runners import Runner\n",
    "from google.genai import types # For creating message Content/Parts\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "# Convenience libraries for working with Neo4j inside of Google ADK\n",
    "from neo4j_for_adk import graphdb\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8adb3fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponse(id='chatcmpl-CStG9EL9bEb7UnsiZ9ra9NAqDQanb', created=1761001637, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_f64f290af2', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Yes, I'm ready. How can I assist you today?\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=13, prompt_tokens=27, total_tokens=40, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n",
      "\n",
      "OpenAI is ready for use.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Model Constants for easier use ---\n",
    "MODEL_GPT = \"openai/gpt-4o\"\n",
    "\n",
    "llm = LiteLlm(model=MODEL_GPT)\n",
    "\n",
    "# Test LLM with a direct call\n",
    "print(llm.llm_client.completion(model=llm.model, \n",
    "                                messages=[{\"role\": \"user\", \"content\": \"Are you ready?\"}], \n",
    "                                tools=[]))\n",
    "\n",
    "print(\"\\nOpenAI is ready for use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e63d464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import make_agent_caller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a678425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hello tool \n",
    "def say_hello(person_name: str) -> dict:\n",
    "    \"\"\"Formats a welcome message to a named person. \n",
    "\n",
    "    Args:\n",
    "        person_name (str): the name of the person saying hello\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the results of the query.\n",
    "              Includes a 'status' key ('success' or 'error').\n",
    "              If 'success', includes a 'query_result' key with an array of result rows.\n",
    "              If 'error', includes an 'error_message' key.\n",
    "    \"\"\"\n",
    "    return graphdb.send_query(\"RETURN 'Hello to you, ' + $person_name AS reply\",\n",
    "    {\n",
    "        \"person_name\": person_name\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36e2a609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new goodbye tool\n",
    "def say_goodbye() -> dict:\n",
    "    \"\"\"Provides a simple farewell message to conclude the conversation.\"\"\"\n",
    "    return graphdb.send_query(\"RETURN 'Goodbye from Cypher!' as farewell\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e256bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Agent 'greeting_subagent_v1' created.\n"
     ]
    }
   ],
   "source": [
    "# --- Greeting Agent ---\n",
    "greeting_subagent = Agent(\n",
    "    model=llm,\n",
    "    name=\"greeting_subagent_v1\",\n",
    "    instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting to the user. \"\n",
    "                \"Use the 'say_hello' tool to generate the greeting. \"\n",
    "                \"If the user provides their name, make sure to pass it to the tool. \"\n",
    "                \"Do not engage in any other conversation or tasks.\",\n",
    "    description=\"Handles simple greetings and hellos using the 'say_hello' tool.\", # Crucial for delegation\n",
    "    tools=[say_hello],\n",
    ")\n",
    "print(f\"✅ Agent '{greeting_subagent.name}' created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78a65b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Agent 'farewell_subagent_v1' created.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Farewell Agent ---\n",
    "farewell_subagent = Agent(\n",
    "    # Can use the same or a different model\n",
    "    model=llm, # Sticking with GPT for this example\n",
    "    name=\"farewell_subagent_v1\",\n",
    "    instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message. \"\n",
    "                \"Use the 'say_goodbye' tool when the user indicates they are leaving or ending the conversation \"\n",
    "                \"(e.g., using words like 'bye', 'goodbye', 'thanks bye', 'see you'). \"\n",
    "                \"Do not perform any other actions.\",\n",
    "    description=\"Handles simple farewells and goodbyes using the 'say_goodbye' tool.\", # Crucial for delegation\n",
    "    tools=[say_goodbye],\n",
    ")\n",
    "print(f\"✅ Agent '{farewell_subagent.name}' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32f65701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Root Agent 'friendly_agent_team_v1' created with sub-agents: ['greeting_subagent_v1', 'farewell_subagent_v1']\n"
     ]
    }
   ],
   "source": [
    "root_agent = Agent(\n",
    "    name=\"friendly_agent_team_v1\", # Give it a new version name\n",
    "    model=llm,\n",
    "    description=\"The main coordinator agent. Delegates greetings/farewells to specialists.\",\n",
    "    instruction=\"\"\"You are the main Agent coordinating a team. Your primary responsibility is to be friendly.\n",
    " \n",
    "                You have specialized sub-agents: \n",
    "                1. 'greeting_agent': Handles simple greetings like 'Hi', 'Hello'. Delegate to it for these. \n",
    "                2. 'farewell_agent': Handles simple farewells like 'Bye', 'See you'. Delegate to it for these. \n",
    "\n",
    "                Analyze the user's query. If it's a greeting, delegate to 'greeting_agent'. \n",
    "                If it's a farewell, delegate to 'farewell_agent'. \n",
    "                \n",
    "                For anything else, respond appropriately or state you cannot handle it.\n",
    "                \"\"\",\n",
    "    tools=[], # No tools for the root agent\n",
    "    # Key change: Link the sub-agents here!\n",
    "    sub_agents=[greeting_subagent, farewell_subagent]\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"✅ Root Agent '{root_agent.name}' created with sub-agents: {[sa.name for sa in root_agent.sub_agents]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4279d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> User Query: Hello I'm ABK\n",
      "  [Event] Author: friendly_agent_team_v1, Type: Event, Final: False, Content: parts=[Part(\n",
      "  function_call=FunctionCall(\n",
      "    args={\n",
      "      'agent_name': 'greeting_subagent_v1'\n",
      "    },\n",
      "    id='call_HRE9ltITNI2w6TYnU1xLaadk',\n",
      "    name='transfer_to_agent'\n",
      "  )\n",
      ")] role='model'\n",
      "  [Event] Author: friendly_agent_team_v1, Type: Event, Final: False, Content: parts=[Part(\n",
      "  function_response=FunctionResponse(\n",
      "    id='call_HRE9ltITNI2w6TYnU1xLaadk',\n",
      "    name='transfer_to_agent',\n",
      "    response={\n",
      "      'result': None\n",
      "    }\n",
      "  )\n",
      ")] role='user'\n",
      "  [Event] Author: greeting_subagent_v1, Type: Event, Final: False, Content: parts=[Part(\n",
      "  function_call=FunctionCall(\n",
      "    args={\n",
      "      'person_name': 'ABK'\n",
      "    },\n",
      "    id='call_X4JMyYWGinUIKIRrBRqcJBC8',\n",
      "    name='say_hello'\n",
      "  )\n",
      ")] role='model'\n",
      "  [Event] Author: greeting_subagent_v1, Type: Event, Final: False, Content: parts=[Part(\n",
      "  function_response=FunctionResponse(\n",
      "    id='call_X4JMyYWGinUIKIRrBRqcJBC8',\n",
      "    name='say_hello',\n",
      "    response={\n",
      "      'query_result': [\n",
      "        {\n",
      "          'reply': 'Hello to you, ABK'\n",
      "        },\n",
      "      ],\n",
      "      'status': 'success'\n",
      "    }\n",
      "  )\n",
      ")] role='user'\n",
      "  [Event] Author: greeting_subagent_v1, Type: Event, Final: True, Content: parts=[Part(\n",
      "  text='Hello to you, ABK!'\n",
      ")] role='model'\n",
      "<<< Agent Response: Hello to you, ABK!\n",
      "\n",
      ">>> User Query: Thanks, bye!\n",
      "  [Event] Author: greeting_subagent_v1, Type: Event, Final: False, Content: parts=[Part(\n",
      "  function_call=FunctionCall(\n",
      "    args={\n",
      "      'agent_name': 'farewell_subagent_v1'\n",
      "    },\n",
      "    id='call_FRtr569uOyq6t0WtfYOXmtjn',\n",
      "    name='transfer_to_agent'\n",
      "  )\n",
      ")] role='model'\n",
      "  [Event] Author: greeting_subagent_v1, Type: Event, Final: False, Content: parts=[Part(\n",
      "  function_response=FunctionResponse(\n",
      "    id='call_FRtr569uOyq6t0WtfYOXmtjn',\n",
      "    name='transfer_to_agent',\n",
      "    response={\n",
      "      'result': None\n",
      "    }\n",
      "  )\n",
      ")] role='user'\n",
      "  [Event] Author: farewell_subagent_v1, Type: Event, Final: False, Content: parts=[Part(\n",
      "  function_call=FunctionCall(\n",
      "    args={},\n",
      "    id='call_M8t110OGVxTufX1YKOMgnOiX',\n",
      "    name='say_goodbye'\n",
      "  )\n",
      ")] role='model'\n",
      "  [Event] Author: farewell_subagent_v1, Type: Event, Final: False, Content: parts=[Part(\n",
      "  function_response=FunctionResponse(\n",
      "    id='call_M8t110OGVxTufX1YKOMgnOiX',\n",
      "    name='say_goodbye',\n",
      "    response={\n",
      "      'query_result': [\n",
      "        {\n",
      "          'farewell': 'Goodbye from Cypher!'\n",
      "        },\n",
      "      ],\n",
      "      'status': 'success'\n",
      "    }\n",
      "  )\n",
      ")] role='user'\n",
      "  [Event] Author: farewell_subagent_v1, Type: Event, Final: True, Content: parts=[Part(\n",
      "  text='Goodbye from Cypher!'\n",
      ")] role='model'\n",
      "<<< Agent Response: Goodbye from Cypher!\n"
     ]
    }
   ],
   "source": [
    "from helper import make_agent_caller\n",
    "\n",
    "root_agent_caller = await make_agent_caller(root_agent)\n",
    "\n",
    "async def run_team_conversation():\n",
    "    await root_agent_caller.call(\"Hello I'm ABK\", True)\n",
    "\n",
    "    await root_agent_caller.call(\"Thanks, bye!\", True)\n",
    "\n",
    "# Execute the conversation using await\n",
    "await run_team_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a067b5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.tools.tool_context import ToolContext\n",
    "\n",
    "def say_hello_stateful(user_name:str, tool_context:ToolContext):\n",
    "    \"\"\"Says hello to the user, recording their name into state.\n",
    "    \n",
    "    Args:\n",
    "        user_name (str): The name of the user.\n",
    "    \"\"\"\n",
    "    tool_context.state[\"user_name\"] = user_name\n",
    "    print(\"\\ntool_context.state['user_name']:\", tool_context.state[\"user_name\"])\n",
    "    return graphdb.send_query(\n",
    "        f\"RETURN 'Hello to you, ' + $user_name + '.' AS reply\",\n",
    "    {\n",
    "        \"user_name\": user_name\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "139a809a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ State-aware 'say_hello_stateful' and 'say_goodbye_stateful' tools defined.\n"
     ]
    }
   ],
   "source": [
    "def say_goodbye_stateful(tool_context: ToolContext) -> dict:\n",
    "    \"\"\"Says goodbye to the user, reading their name from state.\"\"\"\n",
    "    user_name = tool_context.state.get(\"user_name\", \"stranger\")\n",
    "    print(\"\\ntool_context.state['user_name']:\", user_name)\n",
    "    return graphdb.send_query(\"RETURN 'Goodbye, ' + $user_name + ', nice to chat with you!' AS reply\",\n",
    "    {\n",
    "        \"user_name\": user_name\n",
    "    })\n",
    "\n",
    "\n",
    "print(\"✅ State-aware 'say_hello_stateful' and 'say_goodbye_stateful' tools defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ea7c00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Agent 'greeting_agent_stateful_v1' redefined.\n"
     ]
    }
   ],
   "source": [
    "# define a stateful greeting agent. the only difference is that this agent will use the stateful say_hello_stateful tool\n",
    "greeting_agent_stateful = Agent(\n",
    "    model=llm,\n",
    "    name=\"greeting_agent_stateful_v1\",\n",
    "    instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting using the 'say_hello' tool. Do nothing else.\",\n",
    "    description=\"Handles simple greetings and hellos using the 'say_hello_stateful' tool.\",\n",
    "    tools=[say_hello_stateful],\n",
    ")\n",
    "print(f\"✅ Agent '{greeting_agent_stateful.name}' redefined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "054b955e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Agent 'farewell_agent_stateful_v1' redefined.\n"
     ]
    }
   ],
   "source": [
    "farewell_agent_stateful = Agent(\n",
    "    model=llm,\n",
    "    name=\"farewell_agent_stateful_v1\",\n",
    "    instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message using the 'say_goodbye_stateful' tool. Do not perform any other actions.\",\n",
    "    description=\"Handles simple farewells and goodbyes using the 'say_goodbye_stateful' tool.\",\n",
    "    tools=[say_goodbye_stateful],\n",
    ")\n",
    "print(f\"✅ Agent '{farewell_agent_stateful.name}' redefined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1760f6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Root Agent 'friendly_team_stateful' created using agents with stateful tools.\n"
     ]
    }
   ],
   "source": [
    "root_agent_stateful = Agent(\n",
    "    name=\"friendly_team_stateful\", # New version name\n",
    "    model=llm,\n",
    "    description=\"The main coordinator agent. Delegates greetings/farewells to specialists.\",\n",
    "    instruction=\"\"\"You are the main Agent coordinating a team. Your primary responsibility is to be friendly.\n",
    "\n",
    "                You have specialized sub-agents: \n",
    "                1. 'greeting_agent_stateful': Handles simple greetings like 'Hi', 'Hello'. Delegate to it for these. \n",
    "                2. 'farewell_agent_stateful': Handles simple farewells like 'Bye', 'See you'. Delegate to it for these. \n",
    "\n",
    "                Analyze the user's query. If it's a greeting, delegate to 'greeting_agent_stateful'. If it's a farewell, delegate to 'farewell_agent_stateful'. \n",
    "                \n",
    "                For anything else, respond appropriately or state you cannot handle it.\n",
    "                \"\"\",\n",
    "        tools=[], # Still no tools for root\n",
    "        sub_agents=[greeting_agent_stateful, farewell_agent_stateful], # Include sub-agents\n",
    "    )\n",
    "\n",
    "print(f\"✅ Root Agent '{root_agent_stateful.name}' created using agents with stateful tools.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cd2cce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: {}\n"
     ]
    }
   ],
   "source": [
    "root_stateful_caller = await make_agent_caller(root_agent_stateful)\n",
    "\n",
    "session = await root_stateful_caller.get_session()\n",
    "\n",
    "print(f\"Initial State: {session.state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9600fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> User Query: Hello, I'm ABK!\n",
      "\n",
      "tool_context.state['user_name']: ABK\n",
      "<<< Agent Response: Hello to you, ABK.\n",
      "\n",
      ">>> User Query: Thanks, bye!\n",
      "\n",
      "tool_context.state['user_name']: ABK\n",
      "<<< Agent Response: Goodbye, ABK, nice to chat with you!\n",
      "\n",
      "Final State: {'user_name': 'ABK'}\n"
     ]
    }
   ],
   "source": [
    "async def run_stateful_conversation():\n",
    "    await root_stateful_caller.call(\"Hello, I'm ABK!\")\n",
    "\n",
    "    await root_stateful_caller.call(\"Thanks, bye!\")\n",
    "\n",
    "# Execute the conversation using await in an async context (like Colab/Jupyter)\n",
    "await run_stateful_conversation()\n",
    "\n",
    "session = await root_stateful_caller.get_session()\n",
    "\n",
    "print(f\"\\nFinal State: {session.state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "234791de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> User Query: hi goofbye\n",
      "\n",
      "tool_context.state['user_name']: ABK\n",
      "<<< Agent Response: Goodbye, ABK, nice to chat with you!\n",
      "Response: Goodbye, ABK, nice to chat with you!\n",
      "\n",
      ">>> User Query: \n",
      "<<< Agent Response: Thank you for the interaction. If you need anything else, feel free to reach out. Goodbye!\n",
      "Response: Thank you for the interaction. If you need anything else, feel free to reach out. Goodbye!\n",
      "\n",
      ">>> User Query: \n",
      "<<< Agent Response: Goodbye, ABK, nice to chat with you! If there's anything else you need, feel free to reach out. Take care!\n",
      "Response: Goodbye, ABK, nice to chat with you! If there's anything else you need, feel free to reach out. Take care!\n",
      "\n",
      ">>> User Query: \n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Execute the interactive conversation\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m run_interactive_conversation()\n",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m, in \u001b[0;36mrun_interactive_conversation\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_query\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m root_stateful_caller\u001b[38;5;241m.\u001b[39mcall(user_query)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Study/GenAI-Practice/googleADKandNeo4j/helper.py:55\u001b[0m, in \u001b[0;36mAgentCaller.call\u001b[0;34m(self, query, verbose)\u001b[0m\n\u001b[1;32m     51\u001b[0m final_response_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent did not produce a final response.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Default\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Key Concept: run_async executes the agent logic and yields Events.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# We iterate through events to find the final answer.\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mrun_async(user_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_id, session_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id, new_message\u001b[38;5;241m=\u001b[39mcontent):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# You can uncomment the line below to see *all* events during execution\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  [Event] Author: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent\u001b[38;5;241m.\u001b[39mauthor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(event)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Final: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent\u001b[38;5;241m.\u001b[39mis_final_response()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Content: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/studies/lib/python3.12/site-packages/google/adk/runners.py:203\u001b[0m, in \u001b[0;36mRunner.run_async\u001b[0;34m(self, user_id, session_id, new_message, run_config)\u001b[0m\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_new_message_to_session(\n\u001b[1;32m    196\u001b[0m       session,\n\u001b[1;32m    197\u001b[0m       new_message,\n\u001b[1;32m    198\u001b[0m       invocation_context,\n\u001b[1;32m    199\u001b[0m       run_config\u001b[38;5;241m.\u001b[39msave_input_blobs_as_artifacts,\n\u001b[1;32m    200\u001b[0m   )\n\u001b[1;32m    202\u001b[0m invocation_context\u001b[38;5;241m.\u001b[39magent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_agent_to_run(session, root_agent)\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m invocation_context\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mrun_async(invocation_context):\n\u001b[1;32m    204\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m event\u001b[38;5;241m.\u001b[39mpartial:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_service\u001b[38;5;241m.\u001b[39mappend_event(session\u001b[38;5;241m=\u001b[39msession, event\u001b[38;5;241m=\u001b[39mevent)\n",
      "File \u001b[0;32m~/miniconda3/envs/studies/lib/python3.12/site-packages/google/adk/agents/base_agent.py:147\u001b[0m, in \u001b[0;36mBaseAgent.run_async\u001b[0;34m(self, parent_context)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mend_invocation:\n\u001b[1;32m    145\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_async_impl(ctx):\n\u001b[1;32m    148\u001b[0m   \u001b[38;5;28;01myield\u001b[39;00m event\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mend_invocation:\n",
      "File \u001b[0;32m~/miniconda3/envs/studies/lib/python3.12/site-packages/google/adk/agents/llm_agent.py:275\u001b[0m, in \u001b[0;36mLlmAgent._run_async_impl\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_async_impl\u001b[39m(\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28mself\u001b[39m, ctx: InvocationContext\n\u001b[1;32m    274\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncGenerator[Event, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[0;32m--> 275\u001b[0m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm_flow\u001b[38;5;241m.\u001b[39mrun_async(ctx):\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__maybe_save_output_to_state(event)\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "File \u001b[0;32m~/miniconda3/envs/studies/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py:282\u001b[0m, in \u001b[0;36mBaseLlmFlow.run_async\u001b[0;34m(self, invocation_context)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m   last_event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_step_async(invocation_context):\n\u001b[1;32m    283\u001b[0m     last_event \u001b[38;5;241m=\u001b[39m event\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "File \u001b[0;32m~/miniconda3/envs/studies/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py:314\u001b[0m, in \u001b[0;36mBaseLlmFlow._run_one_step_async\u001b[0;34m(self, invocation_context)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Calls the LLM.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m model_response_event \u001b[38;5;241m=\u001b[39m Event(\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mEvent\u001b[38;5;241m.\u001b[39mnew_id(),\n\u001b[1;32m    310\u001b[0m     invocation_id\u001b[38;5;241m=\u001b[39minvocation_context\u001b[38;5;241m.\u001b[39minvocation_id,\n\u001b[1;32m    311\u001b[0m     author\u001b[38;5;241m=\u001b[39minvocation_context\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    312\u001b[0m     branch\u001b[38;5;241m=\u001b[39minvocation_context\u001b[38;5;241m.\u001b[39mbranch,\n\u001b[1;32m    313\u001b[0m )\n\u001b[0;32m--> 314\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m llm_response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_llm_async(\n\u001b[1;32m    315\u001b[0m     invocation_context, llm_request, model_response_event\n\u001b[1;32m    316\u001b[0m ):\n\u001b[1;32m    317\u001b[0m   \u001b[38;5;66;03m# Postprocess after calling the LLM.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_postprocess_async(\n\u001b[1;32m    319\u001b[0m       invocation_context, llm_request, llm_response, model_response_event\n\u001b[1;32m    320\u001b[0m   ):\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;66;03m# Update the mutable event id to avoid conflict\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     model_response_event\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m=\u001b[39m Event\u001b[38;5;241m.\u001b[39mnew_id()\n",
      "File \u001b[0;32m~/miniconda3/envs/studies/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py:539\u001b[0m, in \u001b[0;36mBaseLlmFlow._call_llm_async\u001b[0;34m(self, invocation_context, llm_request, model_response_event)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    535\u001b[0m   \u001b[38;5;66;03m# Check if we can make this llm call or not. If the current call pushes\u001b[39;00m\n\u001b[1;32m    536\u001b[0m   \u001b[38;5;66;03m# the counter beyond the max set value, then the execution is stopped\u001b[39;00m\n\u001b[1;32m    537\u001b[0m   \u001b[38;5;66;03m# right here, and exception is thrown.\u001b[39;00m\n\u001b[1;32m    538\u001b[0m   invocation_context\u001b[38;5;241m.\u001b[39mincrement_llm_call_count()\n\u001b[0;32m--> 539\u001b[0m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m llm_response \u001b[38;5;129;01min\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mgenerate_content_async(\n\u001b[1;32m    540\u001b[0m       llm_request,\n\u001b[1;32m    541\u001b[0m       stream\u001b[38;5;241m=\u001b[39minvocation_context\u001b[38;5;241m.\u001b[39mrun_config\u001b[38;5;241m.\u001b[39mstreaming_mode\n\u001b[1;32m    542\u001b[0m       \u001b[38;5;241m==\u001b[39m StreamingMode\u001b[38;5;241m.\u001b[39mSSE,\n\u001b[1;32m    543\u001b[0m   ):\n\u001b[1;32m    544\u001b[0m     trace_call_llm(\n\u001b[1;32m    545\u001b[0m         invocation_context,\n\u001b[1;32m    546\u001b[0m         model_response_event\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m    547\u001b[0m         llm_request,\n\u001b[1;32m    548\u001b[0m         llm_response,\n\u001b[1;32m    549\u001b[0m     )\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;66;03m# Runs after_model_callback if it exists.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/studies/lib/python3.12/site-packages/google/adk/models/lite_llm.py:778\u001b[0m, in \u001b[0;36mLiteLlm.generate_content_async\u001b[0;34m(self, llm_request, stream)\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m aggregated_llm_response_with_tool_call\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 778\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_client\u001b[38;5;241m.\u001b[39macompletion(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcompletion_args)\n\u001b[1;32m    779\u001b[0m   \u001b[38;5;28;01myield\u001b[39;00m _model_response_to_generate_content_response(response)\n",
      "File \u001b[0;32m~/miniconda3/envs/studies/lib/python3.12/site-packages/google/adk/models/lite_llm.py:101\u001b[0m, in \u001b[0;36mLiteLLMClient.acompletion\u001b[0;34m(self, model, messages, tools, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21macompletion\u001b[39m(\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m, model, messages, tools, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ModelResponse, CustomStreamWrapper]:\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Asynchronously calls acompletion.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    The model response as a message.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m acompletion(\n\u001b[1;32m    102\u001b[0m       model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    103\u001b[0m       messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m    104\u001b[0m       tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    105\u001b[0m       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    106\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/envs/studies/lib/python3.12/site-packages/litellm/utils.py:1374\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper_async\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _caching_handler_response\u001b[38;5;241m.\u001b[39mfinal_embedding_cached_response\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 1374\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m original_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1375\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[1;32m   1377\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   1378\u001b[0m     call_type\u001b[38;5;241m=\u001b[39mcall_type,\n\u001b[1;32m   1379\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/studies/lib/python3.12/site-packages/litellm/main.py:517\u001b[0m, in \u001b[0;36macompletion\u001b[0;34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m ctx \u001b[38;5;241m=\u001b[39m contextvars\u001b[38;5;241m.\u001b[39mcopy_context()\n\u001b[1;32m    515\u001b[0m func_with_context \u001b[38;5;241m=\u001b[39m partial(ctx\u001b[38;5;241m.\u001b[39mrun, func)\n\u001b[0;32m--> 517\u001b[0m init_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mrun_in_executor(\u001b[38;5;28;01mNone\u001b[39;00m, func_with_context)\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init_response, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    519\u001b[0m     init_response, ModelResponse\n\u001b[1;32m    520\u001b[0m ):  \u001b[38;5;66;03m## CACHING SCENARIO\u001b[39;00m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init_response, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "async def run_interactive_conversation():\n",
    "    while True:\n",
    "        user_query = input(\"Ask me something (or type 'exit' to quit): \")\n",
    "        if user_query.lower() == 'exit':\n",
    "            break\n",
    "        response = await root_stateful_caller.call(user_query)\n",
    "        print(f\"Response: {response}\")\n",
    "\n",
    "# Execute the interactive conversation\n",
    "await run_interactive_conversation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (studies)",
   "language": "python",
   "name": "studies"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
